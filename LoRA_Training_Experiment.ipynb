{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1t6mMpUOG9BNcyrhnH3S7bF-JE4GBXjdy","authorship_tag":"ABX9TyMBdGtpUc/TFrRS4995gxMq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 1. Copy the notebook to the root surface again\n","!cp \"drive/MyDrive/Untitled0.ipynb\" ./LoRA_Training_Experiment.ipynb\n","\n","# 2. Strip ALL those scrolling training logs (the 'outputs')\n","# This makes the file small and readable on GitHub\n","!pip install nbconvert\n","!jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True --output LoRA_Training_Experiment.ipynb LoRA_Training_Experiment.ipynb\n","\n","\n","!git add train_lora.py hparams.yaml LoRA_Training_Experiment.ipynb\n","\n","\n","!git commit -m \"Cleaned up notebook logs and stabilized file structure\"\n","!git push origin main --force"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buSCRs01eADr","executionInfo":{"status":"ok","timestamp":1770340118233,"user_tz":0,"elapsed":10072,"user":{"displayName":"Alex Huang","userId":"00458208208392190539"}},"outputId":"c3297fae-4b6a-4275-a699-fc89ba51549f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (7.17.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (4.13.5)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert) (6.3.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.7.1)\n","Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (3.1.6)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.9.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.3.0)\n","Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (3.0.3)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (3.2.0)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.10.4)\n","Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.10.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from nbconvert) (26.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (1.5.1)\n","Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (2.19.2)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.7.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.7->nbconvert) (4.5.1)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.5.0->nbconvert) (7.4.9)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert) (2.21.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert) (4.26.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert) (2.8.3)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert) (4.15.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.30.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (0.4)\n","Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n","Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.1)\n","Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.5.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n","[NbConvertApp] Converting notebook LoRA_Training_Experiment.ipynb to notebook\n","[NbConvertApp] Writing 2423764 bytes to LoRA_Training_Experiment.ipynb\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31m.config/\u001b[m\n","\t\u001b[31mdiffusers/\u001b[m\n","\t\u001b[31mdrive/MyDrive/Untitled0.ipynb\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v2/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v3/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v4/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/checkpoint-1200/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/checkpoint-300/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/checkpoint-600/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/checkpoint-900/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/logs/text2image-fine-tune/1770061495.8036408/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/logs/text2image-fine-tune/events.out.tfevents.1770061495.9000d508f7c0.13534.0\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v5/pytorch_lora_weights.safetensors\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v6/\u001b[m\n","\t\u001b[31mdrive/MyDrive/my_first_lora_output_v7/\u001b[m\n","\t\u001b[31mdrive/MyDrive/training_images/\u001b[m\n","\t\u001b[31mdrive/MyDrive/training_images_experimental/\u001b[m\n","\t\u001b[31mdrive/MyDrive/training_images_padded/\u001b[m\n","\t\u001b[31msample_data/\u001b[m\n","\t\u001b[31muntitled\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n","Everything up-to-date\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"rFtTV8Z6aaCg"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"T3QhSDoIy1VK"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install -q torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/cu126\n","!pip install -q xformers==0.0.33.post2 accelerate transformers bitsandbytes datasets ftfy\n","!git clone https://github.com/huggingface/diffusers.git\n","!pip install -e ./diffusers\n","!pip install mediapipe\n","\n","from accelerate.utils import write_basic_config\n","write_basic_config()\n","\n","print(\"Environment Ready\")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"UxKcso04OBe5"}},{"cell_type":"code","source":["import os\n","from PIL import Image, ImageOps\n","\n","INPUT_FOLDER = \"/content/drive/MyDrive/training_images\"\n","OUTPUT_FOLDER = \"/content/drive/MyDrive/training_images_padded\"\n","\n","if not os.path.exists(OUTPUT_FOLDER):\n","    os.makedirs(OUTPUT_FOLDER)\n","processed_count = 0\n","\n","for filename in os.listdir(INPUT_FOLDER):\n","    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n","        try:\n","            img_path = os.path.join(INPUT_FOLDER, filename)\n","            img = Image.open(img_path).convert(\"RGB\")\n","            img.thumbnail((512, 512), Image.Resampling.LANCZOS)\n","            background = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n","            x_offset = (512 - img.width) // 2\n","            y_offset = (512 - img.height) // 2\n","            background.paste(img, (x_offset, y_offset))\n","            save_path = os.path.join(OUTPUT_FOLDER, filename)\n","            background.save(save_path, quality=100)\n","            processed_count += 1\n","\n","        except Exception as e:\n","            print(f\"Error on {filename}: {e}\")\n","\n","print(f\"\\nPadded images.\")"],"metadata":{"id":"iYVihCmOOEX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","from PIL import Image\n","\n","\n","IMAGES_FOLDER = \"/content/drive/MyDrive/training_images_padded\"\n","TRIGGER_WORD = \"levi_training_model\"\n","\n","metadata_path = os.path.join(IMAGES_FOLDER, \"metadata.jsonl\")\n","\n","image_tags = {\n","    # Bloody/Combat scenes\n","    \"Bloody3.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, blood on face, reverse grip, holding sword, profile, lowered view, intense expression, serious\",\n","    \"Bloody1.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, dual wielding, reverse grip, holding swords, action pose, looking at camera, dynamic angle, mid air, blood on face, injured, intense expression\",\n","    \"Bloody2.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, blood on face, injured, standing, looking at viewer, serious\",\n","\n","    # Generic/Standard Survey Corps Uniform\n","    \"Generic1.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, white cravat, view from the side, standing, hands at sides, serious\",\n","    \"Generic2.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, white cravat, white pants, standing, looking at viewer, hands at sides, serious\",\n","    \"Generic3.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, profile, view from side, looking away, standing, serious\",\n","    \"Generic4.jpeg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, white cravat, profile, view from side, looking away, standing, serious\",\n","    \"Generic5.png\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, standing, looking to the side, hands at sides, serious\",\n","    \"Generic6.jpg\": \"black hair, undercut, gray eyes, male focus, angry, screaming, survey corps uniform, green cape, white cravat, looking at viewer, standing, hands at sides, open mouth\",\n","\n","    # Combat/Sword poses\n","    \"GenericSword1.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, white pants, 3d maneuvering gear, dual wielding, reverse grip, holding swords, action pose, facing camera, dynamic angle, mid air, serious\",\n","    \"GenericSword2.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, white cravat, reverse grip, unsheathing sword, combat pose, looking at viewer, serious\",\n","\n","    # Formal/Suit outfits\n","    \"Suit1.jpg\": \"black hair, undercut, gray eyes, male focus, black suit, white shirt, formal wear, black pants, looking to the side, sitting on chair, serious\",\n","    \"Suit2.jpg\": \"black hair, undercut, gray eyes, male focus, black suit, white shirt, formal wear, looking at viewer, standing, serious\",\n","    \"Suit3.jpg\": \"black hair, undercut, gray eyes, male focus, black suit, white shirt, formal wear, looking at viewer, sitting, view from side, serious\",\n","    \"Suit4.jpg\": \"black hair, undercut, gray eyes, male focus, black suit, white shirt, formal wear, view from side, looking down, obscured eyes, sitting, serious\",\n","\n","    # Casual/Shirt only (no jacket)\n","    \"Shirt1.jpg\": \"black hair, undercut, gray eyes, male focus, survey corps uniform, brown jacket, no cape, white cravat, looking at viewer, serious\",\n","    \"Shirt2.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, rolled sleeves, looking at viewer, standing, arms raised in front, view from side, serious\",\n","    \"Shirt3.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, dual wielding, reverse grip, holding swords, looking at the side, view from side, mid air, 3d maneuvering gear, serious\",\n","    \"Shirt4.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, looking at viewer, view from side, mid air, serious\",\n","    \"Shirt5.jpeg\": \"black hair, undercut, gray eyes, male focus, white shirt, view from side, looking to the side, standing, serious\",\n","    \"Shirt6.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, looking at viewer, sitting on chair, view from side, serious\",\n","    \"Shirt7.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, profile, side view, standing, serious\",\n","    \"Shirt8.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, looking to the side, elevated view, serious\",\n","\n","    # No cape/minimal uniform\n","    \"NoCape1.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, white cravat, looking at ahead, elevated view, serious\",\n","    \"NoCape2.jpg\": \"black hair, undercut, gray eyes, male focus, white shirt, rolled sleeves, white cravat, hands with towel, looking at viewer, view from side, standing, serious\",\n","}\n","\n","valid_images = []\n","print(f\"Scanning {IMAGES_FOLDER}\")\n","\n","if not os.path.exists(IMAGES_FOLDER):\n","    print(f\"Padded folder not found\")\n","else:\n","    for filename in os.listdir(IMAGES_FOLDER):\n","        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n","            try:\n","                path = os.path.join(IMAGES_FOLDER, filename)\n","                with Image.open(path) as img:\n","                    img.verify()\n","                description = image_tags.get(\n","                    filename,\n","                    \"black hair, undercut, gray eyes, male focus, survey corps uniform, green cape, serious\"\n","                )\n","                full_text = f\"{TRIGGER_WORD}, {description}\"\n","\n","                entry = {\"file_name\": filename, \"text\": full_text}\n","                valid_images.append(json.dumps(entry))\n","                print(f\"Mapped: {filename}\")\n","\n","            except Exception as e:\n","                print(f\"Bad file {filename}: {e}\")\n","\n","    if len(valid_images) > 0:\n","        with open(metadata_path, \"w\") as f:\n","            for line in valid_images:\n","                f.write(line + \"\\n\")\n","        print(f\"\\nMetadata saved \")\n","    else:\n","        print(\"No valid images found.\")"],"metadata":{"id":"uGyh2YeW0MSp","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","SOURCE_DIR = \"/content/drive/MyDrive/training_images_padded\"\n","LOCAL_DIR = \"/content/local_train_data\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/my_first_lora_output_v5\"\n","\n","\n","if os.path.exists(LOCAL_DIR):\n","    shutil.rmtree(LOCAL_DIR)\n","shutil.copytree(SOURCE_DIR, LOCAL_DIR)\n","print(f\"Copied to {LOCAL_DIR}\")\n","\n","\n","os.environ[\"MODEL_NAME\"] = \"runwayml/stable-diffusion-v1-5\"\n","os.environ[\"OUTPUT_DIR\"] = OUTPUT_DIR\n","os.environ[\"DATA_DIR\"] = LOCAL_DIR\n","\n","!accelerate launch /content/diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --train_data_dir=$DATA_DIR \\\n","  --dataloader_num_workers=0 \\\n","  --resolution=512 \\\n","  --center_crop \\\n","  --train_batch_size=1 \\\n","  --gradient_accumulation_steps=4 \\\n","  --max_train_steps=900 \\\n","  --learning_rate=1.5e-04 \\\n","  --max_grad_norm=1 \\\n","  --lr_scheduler=\"cosine\" \\\n","  --lr_warmup_steps=0 \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --rank=32 \\\n","  --checkpointing_steps=150 \\\n","  --seed=1337 \\\n","  --mixed_precision=\"fp16\"\n"],"metadata":{"collapsed":true,"id":"6xsIYQgT1lDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n","from PIL import Image\n","from IPython.display import display\n","\n","CHECKPOINT_STEP = 600\n","OUTPUT_DIR = \"/content/drive/MyDrive/my_first_lora_output_v5\"\n","\n","LORA_SCALE = 0.9\n","NUM_IMAGES = 2\n","GUIDANCE_SCALE = 7.0\n","INFERENCE_STEPS = 30\n","\n","ENABLE_HIRES_FIX = True\n","HIRES_STRENGTH = 0.35\n","UPSCALE_FACTOR = 1.5\n","\n","model_folder = os.path.join(OUTPUT_DIR, f\"checkpoint-{CHECKPOINT_STEP}\")\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    torch_dtype=torch.float16,\n","    safety_checker=None,\n","    requires_safety_checker=False\n",").to(\"cuda\")\n","\n","pipe.load_lora_weights(model_folder, weight_name=\"pytorch_lora_weights.safetensors\")\n","\n","refiner = StableDiffusionImg2ImgPipeline(**pipe.components)\n","\n","def image_grid(imgs, rows, cols):\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', (cols*w, rows*h))\n","    for i, img in enumerate(imgs):\n","        grid.paste(img, (i%cols*w, i//cols*h))\n","    return grid\n","\n","prompt = (\n","\"levi_training_model, black hair, undercut, gray eyes, male focus, black suit, white shirt, formal wear, looking at viewer, sitting, view from side, serious, highly detailed eyes, sharp focus\"\n",")\n","\n","neg_prompt = (\n","    \"shiny eyes, glossy eyes, wet eyes, reflective eyes, eye shine, \"\n","    \"bad anatomy, poorly drawn eyes, ugly eyes, lowres, blurry, bad quality\"\n",")\n","\n","print(f\"\\nGenerating Base Images...\")\n","\n","\n","base_images = pipe(\n","    prompt,\n","    negative_prompt=neg_prompt,\n","    num_inference_steps=INFERENCE_STEPS,\n","    guidance_scale=GUIDANCE_SCALE,\n","    cross_attention_kwargs={\"scale\": LORA_SCALE},\n","    num_images_per_prompt=NUM_IMAGES,\n","    height=512, width=512\n",").images\n","\n","final_images = base_images\n","\n","if ENABLE_HIRES_FIX:\n","    print(f\"Running Hires\")\n","    hires_images = []\n","\n","    for img in base_images:\n","        w, h = img.size\n","        new_w, new_h = int(w * UPSCALE_FACTOR), int(h * UPSCALE_FACTOR)\n","        upscaled_img = img.resize((new_w, new_h), resample=Image.LANCZOS)\n","\n","        refined_img = refiner(\n","            prompt=prompt,\n","            negative_prompt=neg_prompt,\n","            image=upscaled_img,\n","            num_inference_steps=30,\n","            strength=HIRES_STRENGTH,\n","            guidance_scale=GUIDANCE_SCALE,\n","            cross_attention_kwargs={\"scale\": LORA_SCALE}\n","        ).images[0]\n","        hires_images.append(refined_img)\n","\n","    final_images = hires_images\n","\n","grid = image_grid(final_images, rows=1, cols=NUM_IMAGES)\n","display(grid)\n","\n","print(\"\\nGeneration complete!\")"],"metadata":{"id":"cgJaLgX_9QLI"},"execution_count":null,"outputs":[]}]}